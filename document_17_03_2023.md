# ECIQC document (17th March 2023)

This is a document for the ECIQC project. It summarises the work done by the 2020 and 2021 cohort, features added by the 2022 cohort (October 2022 to March 2023), work plans for the second half of the year.

### Contents
* Background
* Work by the 2020 and 2021 cohort 
* New features by the 2022 cohort
* Work plans
* Details of different branches and issues

### New features by the 2022 cohort
When we took on this project, we had only a vague idea on how the pipeline actually work, and none of us had any experience working with c++! A lot of ideas went around on what we should do! As time went by, with the help of Martin, Chris, Ian, Julia and Emmanuel, we had a clear road map:

> --> create a logging system for each part
> 
> --> build up the whole pipeline
>    
> --> test the whole pipeline under different cases
>    
> --> debug and improve

Now let's dive deep into each part, and see what we've already done.

#### Step1: Create a logging system

Logging is an important part in modern software engineering, and it helps us get familiar with the code. So we started with adding the logger. Martin taught us how to set up the [DCMTK logger file](./src/logging.cpp). Then we were able to set up the logger by writing
```
set_root_logging("./result/pipeline_root_test.log", true);
```

in the [main function](./exe/qctool.cpp). After that, we could simply add the logging in any part of the codes we were interested in by writing something like
```
OFLOG_ERROR(get_logger(), "Some checks failed!");
```

The `get_logger()` is a function in the [DCMTK logger file](./src/logging.cpp). You can put in any information as the second argument. **Remember** to `#include "logging.hpp"` when you intend to using logging. Also, DTMCK logging system has [five levels of logging severity](https://support.dcmtk.org/docs-dcmrt/classOFLogger.html).

Here's an example of our logger file afterwards (line 1-10).
>INFO - Receiver starts to work!
>
>SCP Port: 11112
>
>title: SCP
>
>INFO - Parser start to work. We're in the processing pipeline!
>
>DEBUG - The key we're working on: (0008,0060)
>
>DEBUG - Process description: "check if modality is MR and manufacturer is 'SIEMENS' or 'HITACHI':"
>
>ERROR - Some checks failed!
>
>INFO - Parser worked finished!
>
>INFO - The parser has finished! Sent to quarantine!

In applications, we need to create a **separate** logger file for **each** image, and one logging file for the pipeline. This is work in progress. Please refer to #73, and the relevant section in `Work plans` and  `Details of different branches and issues` for more information.

#### Step2: build up the whole pipeline
Before we took up this project, different parts of the pipeline ([sender](./src/communication/Sender.cpp), [receiver](./src/communication/Receiver.cpp), [parser](./src/parsing/Parser.cpp),) has been done, and the unit tests works well. Based on that, we need to build up the whole pipeline, and start the pipeline using the executable file. To achieve that, we finish writing the [conductor](./src/Conductor.cpp), and after `cmake ..` and `make`, we can simple write the following code to start the pipeline:

```
./build/exe/qctool --config-file='./schema/PipelineCase.json'
```
The config file needs to be specified to tell the pipeline what to do (e.g. anonymise the patient name, birthday, etc), which port number to listen at and to send the postprocessed files.

The pipeline cannot work on its own. It needs to listen to *someone* to receiver the files, and send the postprocessed files to *someone* who is listening. There are two choices. The first choice is to use the [DCMTK SCP](https://support.dcmtk.org/docs/storescp.html) and [DCMTK SCU](https://support.dcmtk.org/docs/storescu.html) executable files. The second choice is to use [pynetdicom](https://github.com/pydicom/pynetdicom) to manually set up these executable files. Both approaches are wrapped up in the pipelinetest_main.py (./pipelinetest_main.py) file, and we can use the argument `use_pynetdicom` to select the desired one. 

The whole pipeline can run fairly well by running `python pipelinetest_main.py`, but there are three issues worth mentioning

(1) **system compatibility issue**. It works fine on linux, but does not perfectly fit into macOS. So Ola and Alexi are actively working on it to find the bug.
(2) **process killing issue**. Once the pipeline is started in `python pipelinetest_main.py`, we've got no control over them. The current solution is: in the command line, type in
```
ps aux
```
and then find all the previously running processes like
```
weiym97   7002  0.0  1.5 1105972 124536 pts/5  Sl   11:25   0:00 ./build/exe/qctool --config-file=./schema/MultipleConditionC
weiym97   7003  0.0  0.7 624156 57660 pts/5    S    11:25   0:05 python3 tests/TestStorageSCP.py 11113 ./result/
weiym97   7004  0.0  0.6 624156 51616 pts/5    S    11:25   0:05 python3 tests/TestStorageSCP.py 11114 ./result_quarantine/
```
and then kill the processes
```
kill 7002 7003 7004
```
This is quite time consuming. So Ola and Alexi is going to find a better way to kill them. Hopefully, some of the codes newly added to i71-pipeline would help.
(3) **Context presentation issue**. The SCU, SCP, and pipeline are like human beings. They need some language to talk with each other (transfer syntax & abstract syntax), and need to reach an agreement before sending any information (DICOM images). Adding the context presentation is notoriously tricky (see #72 for detailed discussion). The current strategy is to build up minimal 

### Work plans
#### Plan1: Modifying the logging system
Assignee: Ola
Expected finishing time: before June.
Description: Please see #73 for more information. Briefly, we need separate logging for each image, and another logging file for the pipeline. Also, there are many annoying print-outs in different files. They might be useful when debugging, but should be cleared now.
Branch: i73-logging

### Details of different branches and issues
#### #73: 
